{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using dropout and batch normalization non classificational probl"
      ],
      "metadata": {
        "id": "ZVoRv2tgFldz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "spotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n",
        "\n",
        "X = spotify.copy().dropna()\n",
        "y = X.pop('track_popularity')\n",
        "artists = X['track_artist']\n",
        "\n",
        "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
        "                'speechiness', 'acousticness', 'instrumentalness',\n",
        "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
        "features_cat = ['playlist_genre']\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), features_num),\n",
        "    (OneHotEncoder(), features_cat),\n",
        ")\n",
        "\n",
        "def group_split(X, y, group, train_size=0.75):\n",
        "    splitter = GroupShuffleSplit(train_size=train_size)\n",
        "    train, test = next(splitter.split(X, y, groups=group))\n",
        "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "y_train = y_train / 100\n",
        "y_valid = y_valid / 100\n",
        "\n",
        "input_shape = [X_train.shape[1]]\n",
        "print(\"Input shape: {}\".format(input_shape))"
      ],
      "metadata": {
        "id": "333I3rEUFtYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=50,\n",
        "    verbose=0,\n",
        ")"
      ],
      "metadata": {
        "id": "nqi5zMKBF3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "concrete = pd.read_csv('../input/dl-course-data/concrete.csv')\n",
        "df = concrete.copy()\n",
        "\n",
        "df_train = df.sample(frac=0.7, random_state=0)\n",
        "df_valid = df.drop(df_train.index)\n",
        "\n",
        "X_train = df_train.drop('CompressiveStrength', axis=1)\n",
        "X_valid = df_valid.drop('CompressiveStrength', axis=1)\n",
        "y_train = df_train['CompressiveStrength']\n",
        "y_valid = df_valid['CompressiveStrength']\n",
        "\n",
        "input_shape = [X_train.shape[1]]"
      ],
      "metadata": {
        "id": "kMQXDPdCGD5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='mae',\n",
        "    metrics=['mae'],\n",
        ")\n",
        "EPOCHS = 100\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=64,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=0,\n",
        ")"
      ],
      "metadata": {
        "id": "Qp6EgdbSGJbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary classification"
      ],
      "metadata": {
        "id": "gxsK7wwaGO2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('animation', html='html5')"
      ],
      "metadata": {
        "id": "kG_d8TEzGUq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "hotel = pd.read_csv('../input/dl-course-data/hotel.csv')\n",
        "\n",
        "X = hotel.copy()\n",
        "y = X.pop('is_canceled')\n",
        "\n",
        "X['arrival_date_month'] = \\\n",
        "    X['arrival_date_month'].map(\n",
        "        {'January':1, 'February': 2, 'March':3,\n",
        "         'April':4, 'May':5, 'June':6, 'July':7,\n",
        "         'August':8, 'September':9, 'October':10,\n",
        "         'November':11, 'December':12}\n",
        "    )\n",
        "\n",
        "features_num = [\n",
        "    \"lead_time\", \"arrival_date_week_number\",\n",
        "    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n",
        "    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n",
        "    \"is_repeated_guest\", \"previous_cancellations\",\n",
        "    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n",
        "    \"total_of_special_requests\", \"adr\",\n",
        "]\n",
        "features_cat = [\n",
        "    \"hotel\", \"arrival_date_month\", \"meal\",\n",
        "    \"market_segment\", \"distribution_channel\",\n",
        "    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n",
        "]\n",
        "\n",
        "transformer_num = make_pipeline(\n",
        "    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n",
        "    StandardScaler(),\n",
        ")\n",
        "transformer_cat = make_pipeline(\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n",
        "    OneHotEncoder(handle_unknown='ignore'),\n",
        ")\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (transformer_num, features_num),\n",
        "    (transformer_cat, features_cat),\n",
        ")\n",
        "\n",
        "# stratify - make sure classes are evenlly represented across splits\n",
        "X_train, X_valid, y_train, y_valid = \\\n",
        "    train_test_split(X, y, stratify=y, train_size=0.75)\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "\n",
        "input_shape = [X_train.shape[1]]"
      ],
      "metadata": {
        "id": "NLJLf_UQGWpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# YOUR CODE HERE: define the model given in the diagram\n",
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    \n",
        "    layers.Dense(256,activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    \n",
        "    layers.Dense(256,activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    \n",
        "    layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "UetO3KoGGYs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['binary_accuracy'])\n"
      ],
      "metadata": {
        "id": "zIpmGX8QGbjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=200,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")"
      ],
      "metadata": {
        "id": "O2WFUraWGedA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}