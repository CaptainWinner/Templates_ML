{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author identification"
      ],
      "metadata": {
        "id": "6E3wGdYklmwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "df.head(5) # for showing a snapshot of the dataset"
      ],
      "metadata": {
        "id": "RW69pYkwloRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "# Defining a module for Text Processing\n",
        "def text_process(tex):\n",
        "    # 1. Removal of Punctuation Marks \n",
        "    nopunct=[char for char in tex if char not in string.punctuation]\n",
        "    nopunct=''.join(nopunct)\n",
        "    # 2. Lemmatisation \n",
        "    a=''\n",
        "    i=0\n",
        "    for i in range(len(nopunct.split())):\n",
        "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
        "        a=a+b+' '\n",
        "    # 3. Removal of Stopwords\n",
        "    return [word for word in a.split() if word.lower() not \n",
        "            in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "DnFZ191pmA-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "y = df['author']\n",
        "labelencoder = LabelEncoder()\n",
        "y = labelencoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "A-FKZtCNmCn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional;"
      ],
      "metadata": {
        "id": "a1YoV7TsmFfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "X = df['text']\n",
        "wordcloud1 = WordCloud().generate(X[0]) # for EAP\n",
        "wordcloud2 = WordCloud().generate(X[1]) # for HPL\n",
        "wordcloud3 = WordCloud().generate(X[3]) # for MWS \n",
        "print(X[0])\n",
        "print(df['author'][0])\n",
        "plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "plt.show()\n",
        "print(X[1])\n",
        "print(df['author'][1])\n",
        "plt.imshow(wordcloud2, interpolation='bilinear')\n",
        "plt.show()\n",
        "print(X[3])\n",
        "print(df['author'][3])\n",
        "plt.imshow(wordcloud3, interpolation='bilinear')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EzW10qKGmEHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
        "                                  ,test_size=0.2, random_state=1234)\n",
        "# defining the bag-of-words transformer on the text-processed corpus # i.e., text_process() declared in II is executed...\n",
        "bow_transformer=CountVectorizer(analyzer=text_process).fit(X_train)\n",
        "# transforming into Bag-of-Words and hence textual data to numeric..\n",
        "text_bow_train=bow_transformer.transform(X_train)#ONLY TRAINING DATA\n",
        "# transforming into Bag-of-Words and hence textual data to numeric..\n",
        "text_bow_test=bow_transformer.transform(X_test)#TEST DATA"
      ],
      "metadata": {
        "id": "lc4DPWhfmGkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# instantiating the model with Multinomial Naive Bayes..\n",
        "model = MultinomialNB()\n",
        "# training the model...\n",
        "model = model.fit(text_bow_train, y_train)"
      ],
      "metadata": {
        "id": "RRlfaRLzmJMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(text_bow_train, y_train)"
      ],
      "metadata": {
        "id": "knRgaMWImJ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(text_bow_test, y_test)"
      ],
      "metadata": {
        "id": "TA7NOI7SmK9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.metrics import classification_report\n",
        " \n",
        "# getting the predictions of the Validation Set...\n",
        "predictions = model.predict(text_bow_test)\n",
        "# getting the Precision, Recall, F1-Score\n",
        "print(classification_report(y_test,predictions))"
      ],
      "metadata": {
        "id": "vnTWwy8TmL7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix, optional:"
      ],
      "metadata": {
        "id": "qFOPdIAVmO0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "# Defining a module for Confusion Matrix...\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "print(cm)\n",
        "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0])\n",
        "                                  , range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "cm = confusion_matrix(y_test,predictions)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, classes=[0,1,2], normalize=True,\n",
        "                      title='Confusion Matrix')"
      ],
      "metadata": {
        "id": "S_mCJ7sQmN3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgUauB_HmRKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}