{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Binary classification for images"
      ],
      "metadata": {
        "id": "Tk9IeUaJ3FhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка датасета"
      ],
      "metadata": {
        "id": "DVwSaU4N3Jlc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFx5TOc13E4p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "metadata": {
        "id": "bmUGOARs5c6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если по ссылке:"
      ],
      "metadata": {
        "id": "Y106RLsu3iVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O \\\n",
        "   /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "id": "J0vHIdoW3Nks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если локально:"
      ],
      "metadata": {
        "id": "UnsfSF-v3koU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZIP:"
      ],
      "metadata": {
        "id": "GjpEVXw63rQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip' #файл в формате zip\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r') #открываем для чтения\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered' #это папка в архиве\n",
        "train_dir = os.path.join(base_dir, 'train') #это папка в папке в архиве\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') #папка в папке в папке\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        " \n",
        "train_cat_fnames = os.listdir(train_cats_dir) #список имен файлов в каталоге.\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n"
      ],
      "metadata": {
        "id": "V4AuWrLi3mXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Локально отдельные файлы"
      ],
      "metadata": {
        "id": "OjHrVPCX4fbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_ = image_dataset_from_directory(\n",
        "    '../input/car-or-truck/train', #ссылка на файл\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=[128, 128], #размер изображения\n",
        "    interpolation='nearest',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        ")\n",
        "ds_valid_ = image_dataset_from_directory(\n",
        "    '../input/car-or-truck/valid', #ссылка на файл\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=[128, 128],\n",
        "    interpolation='nearest',\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "5nQ0Wj4M4VR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка данных"
      ],
      "metadata": {
        "id": "JGd78g5x5Lh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "rbszCbDO4U0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "или так"
      ],
      "metadata": {
        "id": "8IAaeKQd5Zas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Pipeline\n",
        "def convert_to_float(image, label):\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    return image, label\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "ds_train = (\n",
        "    ds_train_\n",
        "    .map(convert_to_float)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "ds_valid = (\n",
        "    ds_valid_\n",
        "    .map(convert_to_float)\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "NBS9BNXu5Rsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа с моделью"
      ],
      "metadata": {
        "id": "dUOEOs4i5t_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если загружать предобученные веса"
      ],
      "metadata": {
        "id": "r2yjglOF54xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception V3:"
      ],
      "metadata": {
        "id": "xyeV8Fa26Inj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 "
      ],
      "metadata": {
        "id": "fIjWpDfy5xe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = InceptionV3(\n",
        "    input_shape=(150, 150, 3), include_top=False, weights=None)\n",
        "pre_trained_model.load_weights(local_weights_file)"
      ],
      "metadata": {
        "id": "Cc-8Ii1z6NTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если без изменения весов:"
      ],
      "metadata": {
        "id": "vYUaIcyv6Sky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "_TJ80rL56SBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape:', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "metadata": {
        "id": "-fGHJWdH6Z-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Configure and compile the model\n",
        "model = Model(pre_trained_model.input, x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.0001),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "AfsjLfbu6aoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Или так"
      ],
      "metadata": {
        "id": "izQRFxODMajn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.Sequential([\n",
        "    pre_trained_model.input,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.0001),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "NWl8X6ZC6i_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=2,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ],
      "metadata": {
        "id": "CiYePH6x70tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With fine-tuning: (only for top layers, after mix7)"
      ],
      "metadata": {
        "id": "u0UDkVlX74Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "unfreeze = False\n",
        "\n",
        "# Unfreeze all models after \"mixed6\"\n",
        "for layer in pre_trained_model.layers:\n",
        "  if unfreeze:\n",
        "    layer.trainable = True\n",
        "  if layer.name == 'mixed6':\n",
        "    unfreeze = True\n",
        "\n",
        "# As an optimizer, here we will use SGD \n",
        "# with a very low learning rate (0.00001)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(\n",
        "                  lr=0.00001, \n",
        "                  momentum=0.9),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "5oIa8UIR75TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=50,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ],
      "metadata": {
        "id": "fLfPw38e8IAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting:"
      ],
      "metadata": {
        "id": "jVOeB_2N8Rjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Retrieve a list of accuracy results on training and validation data\n",
        "# sets for each training epoch\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "# Retrieve a list of list results on training and validation data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "metadata": {
        "id": "5v4RQWN68SoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С предобученной моделью по другому"
      ],
      "metadata": {
        "id": "Bepsxg46LklO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import model смотри в TensorHub"
      ],
      "metadata": {
        "id": "21yGOG1GLmee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_base.trainable = False"
      ],
      "metadata": {
        "id": "mrV9OY75Lqyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    pretrained_base,\n",
        "    layers.Flatten(),\n",
        "    # YOUR CODE HERE. Attach a head of dense layers.\n",
        "    layers.Dense(6,activation='relu'),\n",
        "    layers.Dense(1,activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "NoLEKJU7LtSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "Rh7D1IW2LvCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_valid,\n",
        "    epochs=30,\n",
        ")"
      ],
      "metadata": {
        "id": "rfrXod49LwOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
      ],
      "metadata": {
        "id": "N02U8fqDLx_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Без предобученной модели:"
      ],
      "metadata": {
        "id": "uZidGTtD8TTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
        "    \n",
        "    # Data Augmentation\n",
        "    preprocessing.RandomContrast(factor=0.10),\n",
        "    preprocessing.RandomFlip(mode='horizontal'),\n",
        "    preprocessing.RandomRotation(factor=0.10),\n",
        "    # Block One\n",
        "    layers.BatchNormalization(renorm=True),\n",
        "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    # Block Two\n",
        "    layers.BatchNormalization(renorm=True),\n",
        "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    # Block Three\n",
        "    layers.BatchNormalization(renorm=True),\n",
        "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPool2D(),\n",
        "\n",
        "    # Head\n",
        "    layers.BatchNormalization(renorm=True),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "metadata": {
        "id": "CosMYQBx8Whw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_valid,\n",
        "    epochs=50,\n",
        ")\n",
        "\n",
        "# Plot learning curves\n",
        "import pandas as pd\n",
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
      ],
      "metadata": {
        "id": "z8-D85t48ZQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('image', cmap='magma')\n",
        "warnings.filterwarnings(\"ignore\") # to clean up output cells"
      ],
      "metadata": {
        "id": "7gmagdC58miH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stops"
      ],
      "metadata": {
        "id": "55ceo1tV9I3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import callbacks"
      ],
      "metadata": {
        "id": "fIT8ILoxL94d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = callbacks.EarlyStopping(min_delta=0.001,\n",
        "                                        patience=5,\n",
        "                                        restore_best_weights=True)"
      ],
      "metadata": {
        "id": "Pmd9HT9AMAF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
        "    layers.Dropout(0.3), #!!!!!!\n",
        "    layers.Dense(64, activation='relu'),    \n",
        "    layers.Dense(1)\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping] #!!!!!!\n",
        ")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
        "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
      ],
      "metadata": {
        "id": "vJsCrmzNMEY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}